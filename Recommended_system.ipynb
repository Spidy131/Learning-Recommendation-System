{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT35tojinsf3",
        "outputId": "86fc4c44-98e8-462d-b20e-79b1dd7d4cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Interactions: (1000, 13)\n",
            "Items: (38, 3)\n",
            "\n",
            "Users: 198\n",
            "Items: 150\n",
            "Interactions: 1000\n",
            "\n",
            "Action counts:\n",
            "action\n",
            "viewed       211\n",
            "completed    207\n",
            "started      198\n",
            "skipped      196\n",
            "attempted    188\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Time range:\n",
            "2025-01-01 00:44:59 â†’ 2025-12-13 18:20:13\n",
            "\n",
            "Encoded Users: 198 Encoded Items: 150\n",
            "Training samples: 802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 47.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.0334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 45.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 4.9547\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 45.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.8933\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 45.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 4.8317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:00<00:00, 51.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 4.7705\n",
            "\n",
            "Model saved as: gru_model.pt\n",
            "\n",
            "Enter User ID (example: U0142): U0142\n",
            "\n",
            "ðŸ§‘ User ID: U0142\n",
            "ðŸ“Œ Current Topic: Probability Distributions\n",
            "\n",
            "ðŸ”® Next Recommended Topics:\n",
            "1. Descriptive Statistics\n",
            "2. Control Flow and Loops\n",
            "3. Hypothesis Testing\n",
            "4. Introduction to Robotics\n",
            "5. Probability Distributions\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 1. IMPORTS\n",
        "# ==========================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ==========================================================\n",
        "# 2. LOAD DATA\n",
        "# ==========================================================\n",
        "interactions = pd.read_csv(\"/learning_interactions_dataset_updated.csv\")\n",
        "items = pd.read_csv(\"/python learning path.csv\")\n",
        "\n",
        "# Ensure original user IDs like U0142 are strings\n",
        "interactions[\"user_id\"] = interactions[\"user_id\"].astype(str)\n",
        "\n",
        "interactions[\"timestamp\"] = pd.to_datetime(interactions[\"timestamp\"])\n",
        "\n",
        "print(\"Interactions:\", interactions.shape)\n",
        "print(\"Items:\", items.shape)\n",
        "\n",
        "# ==========================================================\n",
        "# 3. BASIC EDA\n",
        "# ==========================================================\n",
        "print(\"\\nUsers:\", interactions[\"user_id\"].nunique())\n",
        "print(\"Items:\", interactions[\"item_id\"].nunique())\n",
        "print(\"Interactions:\", len(interactions))\n",
        "\n",
        "print(\"\\nAction counts:\")\n",
        "print(interactions[\"action\"].value_counts())\n",
        "\n",
        "print(\"\\nTime range:\")\n",
        "print(interactions[\"timestamp\"].min(), \"â†’\", interactions[\"timestamp\"].max())\n",
        "\n",
        "# ==========================================================\n",
        "# 4. FEATURE ENGINEERING\n",
        "# ==========================================================\n",
        "# implicit label\n",
        "interactions[\"label\"] = (\n",
        "    interactions[\"action\"].isin([\"completed\",\"attempted\"]) |\n",
        "    (interactions[\"rating_score\"] > 0)\n",
        ").astype(int)\n",
        "\n",
        "# time features\n",
        "interactions[\"hour\"] = interactions[\"timestamp\"].dt.hour\n",
        "interactions[\"dow\"] = interactions[\"timestamp\"].dt.dayofweek\n",
        "\n",
        "# merge metadata\n",
        "interactions = interactions.merge(\n",
        "    items,\n",
        "    left_on=\"course_topic\",\n",
        "    right_on=\"Topic\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# sort\n",
        "interactions = interactions.sort_values([\"user_id\",\"timestamp\"])\n",
        "\n",
        "# encode ids, keep original strings\n",
        "users = interactions[\"user_id\"].unique().tolist()\n",
        "items_list = interactions[\"item_id\"].unique().tolist()\n",
        "\n",
        "user2idx = {u:i for i,u in enumerate(users)}\n",
        "idx2user = {i:u for u,i in user2idx.items()}\n",
        "\n",
        "item2idx = {i:j for j,i in enumerate(items_list)}\n",
        "idx2item = {v:k for k,v in item2idx.items()}\n",
        "\n",
        "interactions[\"u\"] = interactions[\"user_id\"].map(user2idx)\n",
        "interactions[\"i\"] = interactions[\"item_id\"].map(item2idx)\n",
        "\n",
        "n_users = len(user2idx)\n",
        "n_items = len(item2idx)\n",
        "\n",
        "# mapping item index â†’ topic\n",
        "itemid_to_topic = interactions.set_index(\"item_id\")[\"course_topic\"].to_dict()\n",
        "idx2topic = {idx: itemid_to_topic[itemid] for idx, itemid in idx2item.items()}\n",
        "\n",
        "print(\"\\nEncoded Users:\", n_users, \"Encoded Items:\", n_items)\n",
        "\n",
        "# ==========================================================\n",
        "# 5. DATASET CLASS â€” SEQUENCE BUILDER\n",
        "# ==========================================================\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, df, max_len=20):\n",
        "        self.samples = []\n",
        "        df = df.sort_values([\"u\",\"timestamp\"])\n",
        "        for u, grp in df.groupby(\"u\"):\n",
        "            items = grp[\"i\"].tolist()\n",
        "            for t in range(1, len(items)):\n",
        "                seq = items[max(0, t-max_len):t]\n",
        "                target = items[t]\n",
        "                self.samples.append((seq, target))\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        seq, target = self.samples[idx]\n",
        "        seq = np.array(seq, dtype=np.int64)\n",
        "        pad_len = self.max_len - len(seq)\n",
        "        if pad_len > 0:\n",
        "            seq = np.pad(seq, (pad_len, 0))\n",
        "        return torch.tensor(seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "train_ds = SeqDataset(interactions)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "print(\"Training samples:\", len(train_ds))\n",
        "\n",
        "# ==========================================================\n",
        "# 6. GRU MODEL\n",
        "# ==========================================================\n",
        "class GRURec(nn.Module):\n",
        "    def __init__(self, n_items, embed_dim=64, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(n_items, embed_dim)\n",
        "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_dim, n_items)\n",
        "\n",
        "    def forward(self, seq):\n",
        "        e = self.emb(seq)\n",
        "        _, h = self.gru(e)\n",
        "        return self.out(h[-1])\n",
        "\n",
        "model = GRURec(n_items).to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================================================\n",
        "# 7. TRAINING LOOP\n",
        "# ==========================================================\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for seq, tgt in tqdm(train_loader, desc=f\"Epoch {epoch}\"):\n",
        "        seq, tgt = seq.to(device), tgt.to(device)\n",
        "\n",
        "        logits = model(seq)\n",
        "        loss = loss_fn(logits, tgt)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch} Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"gru_model.pt\")\n",
        "print(\"\\nModel saved as: gru_model.pt\")\n",
        "\n",
        "# ==========================================================\n",
        "# 8. RECOMMENDATION FUNCTION (TOPICS)\n",
        "# ==========================================================\n",
        "def recommend_next_topics(user_id, k=5):\n",
        "\n",
        "    user_id = str(user_id)   # Ensure string like U0142\n",
        "\n",
        "    df = interactions[interactions[\"user_id\"] == user_id].sort_values(\"timestamp\")\n",
        "\n",
        "    # Need at least 2 interactions\n",
        "    if len(df) < 2:\n",
        "        return None\n",
        "\n",
        "    # current topic\n",
        "    current_item_id = df.iloc[-1][\"item_id\"]\n",
        "    current_topic = itemid_to_topic.get(current_item_id, \"Unknown\")\n",
        "\n",
        "    # build sequence\n",
        "    seq = df[\"i\"].tolist()[-20:]\n",
        "    seq = np.pad(seq, (20-len(seq), 0))\n",
        "    seq = torch.tensor(seq, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    # predict\n",
        "    logits = model(seq)[0].detach().cpu().numpy()\n",
        "    topk = np.argsort(-logits)[:k]\n",
        "\n",
        "    next_topics = [idx2topic[i] for i in topk]\n",
        "    return current_topic, next_topics\n",
        "\n",
        "# ==========================================================\n",
        "# 9. NICE PRINT FUNCTION\n",
        "# ==========================================================\n",
        "def get_user_current_and_next(user_id):\n",
        "    result = recommend_next_topics(user_id, k=5)\n",
        "\n",
        "    if result is None:\n",
        "        print(f\"\\nâŒ User {user_id} does NOT have enough interactions (need â‰¥2).\")\n",
        "        return\n",
        "\n",
        "    current_topic, next_topics = result\n",
        "\n",
        "    print(f\"\\nðŸ§‘ User ID: {user_id}\")\n",
        "    print(f\"ðŸ“Œ Current Topic: {current_topic}\")\n",
        "\n",
        "    print(\"\\nðŸ”® Next Recommended Topics:\")\n",
        "    for i, t in enumerate(next_topics, 1):\n",
        "        print(f\"{i}. {t}\")\n",
        "\n",
        "# ==========================================================\n",
        "# 10. ASK USER FOR REAL ID LIKE U0142\n",
        "# ==========================================================\n",
        "user_input = input(\"\\nEnter User ID (example: U0142): \")\n",
        "get_user_current_and_next(user_input)\n"
      ]
    }
  ]
}